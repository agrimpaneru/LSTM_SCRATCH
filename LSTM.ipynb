{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a832658f-4e8d-46f0-84b9-6d7f6a0eed0a",
   "metadata": {},
   "source": [
    "## LSTM FROM SCRATCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dba5eda-67ef-4255-a2b4-f571022272fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "e6b6e0a0-9e45-4e23-9983-cb506fb52ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "orgi=open(\"orwell.txt\").read().lower()\n",
    "data=orgi.replace(\"\\n\",\" \")\n",
    "data=data.split(\" \")\n",
    "\n",
    "char2idx={char:i for i,char in enumerate(set(data))}\n",
    "# char2idx['.']=0\n",
    "idx2char={i:char for i,char in enumerate(set(data))}\n",
    "# idx2char[0]=\".\"\n",
    "x=[]\n",
    "for i in orgi.split(\"\\n\"):\n",
    "    x.append([char2idx[j] for j in i.split(\" \")])\n",
    "\n",
    "X_train = []  \n",
    "Y_train = []  \n",
    "\n",
    "for sequence in x:\n",
    "    for i in range(1, len(sequence)-1):\n",
    "        \n",
    "        X_train.append(sequence[:i])\n",
    "        \n",
    "        Y_train.append(sequence[i])\n",
    "max_len=max([len(i) for i in X_train])\n",
    "vocab_size=len(set(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "d21f9d49-89fa-4ebe-a96c-44386b2021fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'reaching': 0,\n",
       " 'five,': 1,\n",
       " 'seen': 2,\n",
       " 'dressed': 3,\n",
       " 'clothes': 4,\n",
       " 'smokeless': 5,\n",
       " 'cruelty': 6,\n",
       " 'light,': 7,\n",
       " 'coat': 8,\n",
       " 'per': 9,\n",
       " 'shoving': 10,\n",
       " 'wandering': 11,\n",
       " 'captivity.': 12,\n",
       " 'cut': 13,\n",
       " 'stone-floored': 14,\n",
       " 'smoking,': 15,\n",
       " 'medical': 16,\n",
       " 'supperâ€”half': 17,\n",
       " 'his': 18,\n",
       " 'happens': 19,\n",
       " 'forty-nine': 20,\n",
       " 'beer': 21,\n",
       " 'hiding': 22,\n",
       " 'twenty': 23,\n",
       " 'went': 24,\n",
       " 'honour': 25,\n",
       " 'black': 26,\n",
       " 'died': 27,\n",
       " 'shivering': 28,\n",
       " 'wen': 29,\n",
       " 'astonishing;': 30,\n",
       " \"mate,'\": 31,\n",
       " 'garments,': 32,\n",
       " 'cold': 33,\n",
       " 'marching': 34,\n",
       " 'stone': 35,\n",
       " 'slices': 36,\n",
       " 'tables.': 37,\n",
       " 'cook': 38,\n",
       " 'managed': 39,\n",
       " 'first': 40,\n",
       " 'panacea': 41,\n",
       " 'hard': 42,\n",
       " \"major's\": 43,\n",
       " 'underwear': 44,\n",
       " 'overnight,': 45,\n",
       " 'worse': 46,\n",
       " 'herculean': 47,\n",
       " 'bags': 48,\n",
       " 'souls.': 49,\n",
       " 'each': 50,\n",
       " '11.': 51,\n",
       " 'appeared': 52,\n",
       " 'us.': 53,\n",
       " 'pot-bellied,': 54,\n",
       " 'benches,': 55,\n",
       " 'smelt': 56,\n",
       " 'lazarus': 57,\n",
       " 'never': 58,\n",
       " 'water': 59,\n",
       " 'elbow': 60,\n",
       " 'buried': 61,\n",
       " 'were': 62,\n",
       " 'misconducted': 63,\n",
       " 'passage.': 64,\n",
       " 'agreed,': 65,\n",
       " 'sprawled': 66,\n",
       " 'tobacco': 67,\n",
       " 'secrets': 68,\n",
       " 'locked': 69,\n",
       " 'him': 70,\n",
       " 'manner': 71,\n",
       " 'carry': 72,\n",
       " 'golden': 73,\n",
       " 'register': 74,\n",
       " 'because': 75,\n",
       " 'margarine,': 76,\n",
       " 'existence.': 77,\n",
       " 'hurried': 78,\n",
       " 'collections': 79,\n",
       " 'all': 80,\n",
       " 'tramps': 81,\n",
       " 'talk': 82,\n",
       " 'each,': 83,\n",
       " 'smallpox.': 84,\n",
       " 'bellies': 85,\n",
       " 'hands': 86,\n",
       " 'looked.': 87,\n",
       " 'cigarette.': 88,\n",
       " 'hunger': 89,\n",
       " 'before': 90,\n",
       " 'who': 91,\n",
       " 'pipes.': 92,\n",
       " 'sang': 93,\n",
       " 'filled': 94,\n",
       " 'picked': 95,\n",
       " 'idiot': 96,\n",
       " 'pine': 97,\n",
       " 'floated': 98,\n",
       " 'sock': 99,\n",
       " 'without': 100,\n",
       " 'ten': 101,\n",
       " 'military-minded': 102,\n",
       " 'watering': 103,\n",
       " 'young,': 104,\n",
       " 'heard': 105,\n",
       " 'suck': 106,\n",
       " 'call': 107,\n",
       " 'after': 108,\n",
       " 'smuggling': 109,\n",
       " 'out': 110,\n",
       " 'seashore.': 111,\n",
       " 'faces,': 112,\n",
       " 'flat': 113,\n",
       " 'said:': 114,\n",
       " 'tramp.': 115,\n",
       " 'limewashed': 116,\n",
       " 'night': 117,\n",
       " 'silent,': 118,\n",
       " 'deliberate': 119,\n",
       " 'lump': 120,\n",
       " 'able': 121,\n",
       " 'useless.': 122,\n",
       " 'surrender': 123,\n",
       " 'consisting': 124,\n",
       " 'way': 125,\n",
       " 'competing': 126,\n",
       " 'tea.': 127,\n",
       " 'water,': 128,\n",
       " 'sight,': 129,\n",
       " 'beef,': 130,\n",
       " 'smoking': 131,\n",
       " 'same,': 132,\n",
       " 'corpse': 133,\n",
       " 'bench,': 134,\n",
       " 'again': 135,\n",
       " 'week-end.': 136,\n",
       " 'to.': 137,\n",
       " 'salvation': 138,\n",
       " 'imbecile': 139,\n",
       " 'paupers': 140,\n",
       " 'bath,': 141,\n",
       " \"tramp's\": 142,\n",
       " 'set': 143,\n",
       " 'yelling': 144,\n",
       " 'spot': 145,\n",
       " 'tea,': 146,\n",
       " 'pitched': 147,\n",
       " 'boiled': 148,\n",
       " 'whimper': 149,\n",
       " 'subjects;': 150,\n",
       " 'seven': 151,\n",
       " 'back': 152,\n",
       " 'sound': 153,\n",
       " 'outbreak': 154,\n",
       " 'quarters': 155,\n",
       " 'breakfast': 156,\n",
       " 'late-afternoon.': 157,\n",
       " 'petty': 158,\n",
       " 'obscenities.': 159,\n",
       " 'work.': 160,\n",
       " 'workhouse': 161,\n",
       " 'he': 162,\n",
       " 'food': 163,\n",
       " 'follow': 164,\n",
       " 'disgusting': 165,\n",
       " 'and': 166,\n",
       " 'dragged': 167,\n",
       " 'potatoes,': 168,\n",
       " 'imaginary': 169,\n",
       " \"'well,\": 170,\n",
       " 'cent': 171,\n",
       " 'looking': 172,\n",
       " 'jobs': 173,\n",
       " 'contraband': 174,\n",
       " 'within': 175,\n",
       " 'others': 176,\n",
       " 'dirt.': 177,\n",
       " 'gods': 178,\n",
       " 'was': 179,\n",
       " 'search': 180,\n",
       " 'boredom.': 181,\n",
       " 'rolled-up': 182,\n",
       " 'socks,': 183,\n",
       " 'idleness.': 184,\n",
       " \"'that's\": 185,\n",
       " 'feet.': 186,\n",
       " 'bad': 187,\n",
       " 'half': 188,\n",
       " 'forty-eight': 189,\n",
       " 'stench': 190,\n",
       " 'rottenness': 191,\n",
       " 'you,': 192,\n",
       " 'medicine,': 193,\n",
       " 'that': 194,\n",
       " 'kind': 195,\n",
       " 'best': 196,\n",
       " 'tub': 197,\n",
       " 'got': 198,\n",
       " 'only': 199,\n",
       " 'one': 200,\n",
       " 'chests,': 201,\n",
       " 'greasy,': 202,\n",
       " 'full': 203,\n",
       " 'truss,': 204,\n",
       " 'trousers': 205,\n",
       " \"guv'nor,'\": 206,\n",
       " 'packed': 207,\n",
       " 'week.': 208,\n",
       " 'even': 209,\n",
       " 'littered': 210,\n",
       " 'took': 211,\n",
       " 'red,': 212,\n",
       " 'bed,': 213,\n",
       " 'help': 214,\n",
       " 'side,': 215,\n",
       " 'being': 216,\n",
       " 'seized,': 217,\n",
       " 'coyly': 218,\n",
       " 'grey': 219,\n",
       " 'room.': 220,\n",
       " 'nearly': 221,\n",
       " 'stuff': 222,\n",
       " 'worthy': 223,\n",
       " 'squalid': 224,\n",
       " 'in.': 225,\n",
       " 'scene,': 226,\n",
       " 'whether': 227,\n",
       " 'work': 228,\n",
       " \"eightpence!'\": 229,\n",
       " 'musclesâ€”every': 230,\n",
       " 'advised': 231,\n",
       " 'food.': 232,\n",
       " 'gates': 233,\n",
       " 'bedsteads': 234,\n",
       " 'seasonâ€”a': 235,\n",
       " 'state': 236,\n",
       " 'god': 237,\n",
       " 'moment,': 238,\n",
       " 'broad': 239,\n",
       " 'it': 240,\n",
       " 'malformation': 241,\n",
       " 'my': 242,\n",
       " 'fragmentary': 243,\n",
       " 'but': 244,\n",
       " 'compassion,': 245,\n",
       " 'table,': 246,\n",
       " 'love': 247,\n",
       " 'six,': 248,\n",
       " 'throw': 249,\n",
       " 'bare': 250,\n",
       " 'library.': 251,\n",
       " 'doors': 252,\n",
       " 'noxious': 253,\n",
       " 'apparatus': 254,\n",
       " 'tread,': 255,\n",
       " 'fair': 256,\n",
       " 'witless': 257,\n",
       " 'miserable': 258,\n",
       " 'job': 259,\n",
       " 'which': 260,\n",
       " 'cases': 261,\n",
       " 'round': 262,\n",
       " 'sunken': 263,\n",
       " 'clouds': 264,\n",
       " 'workhouse,': 265,\n",
       " 'native': 266,\n",
       " 'others,': 267,\n",
       " 'together': 268,\n",
       " 'woman': 269,\n",
       " 'toad.': 270,\n",
       " 'clear': 271,\n",
       " 'away': 272,\n",
       " \"'i\": 273,\n",
       " 'about,': 274,\n",
       " 'exposed;': 275,\n",
       " 'face': 276,\n",
       " 'day.': 277,\n",
       " 'educated': 278,\n",
       " 'moocher,': 279,\n",
       " 'end': 280,\n",
       " 'hoping': 281,\n",
       " 'waist': 282,\n",
       " 'home-made': 283,\n",
       " 'believe,': 284,\n",
       " \"ship's\": 285,\n",
       " 'gone': 286,\n",
       " 'feeling': 287,\n",
       " 'avoid': 288,\n",
       " 'arrived,': 289,\n",
       " 'sacrifice': 290,\n",
       " 'comfortably': 291,\n",
       " 'nothing,': 292,\n",
       " 'it,': 293,\n",
       " 'marking': 294,\n",
       " 'look': 295,\n",
       " 'serve': 296,\n",
       " 'charitable': 297,\n",
       " 'eaten.': 298,\n",
       " 'spikes': 299,\n",
       " 'gruff,': 300,\n",
       " 'little': 301,\n",
       " 'are,': 302,\n",
       " 'shouting': 303,\n",
       " 'talk,': 304,\n",
       " 'how': 305,\n",
       " 'constantly': 306,\n",
       " 'degenerate': 307,\n",
       " 'breakfast.': 308,\n",
       " 'tales': 309,\n",
       " 'grime,': 310,\n",
       " 'odours': 311,\n",
       " 'secure,': 312,\n",
       " 'sit': 313,\n",
       " 'ex-fishermen': 314,\n",
       " 'meal,': 315,\n",
       " 'ineradicably': 316,\n",
       " 'meal.': 317,\n",
       " 'lot': 318,\n",
       " 'jostling': 319,\n",
       " 'deceptive': 320,\n",
       " 'disgraced.': 321,\n",
       " 'imbecile,': 322,\n",
       " 'entered': 323,\n",
       " 'straw': 324,\n",
       " 'already': 325,\n",
       " 'sad': 326,\n",
       " 'vegetables,': 327,\n",
       " 'since': 328,\n",
       " 'a': 329,\n",
       " 'packing': 330,\n",
       " 'said.': 331,\n",
       " 'clothes,': 332,\n",
       " 'shelf,': 333,\n",
       " 'measured': 334,\n",
       " 'potatoes': 335,\n",
       " 'sight.': 336,\n",
       " 'windows': 337,\n",
       " 'unmerciful': 338,\n",
       " 'man': 339,\n",
       " 'waiting': 340,\n",
       " 'tea': 341,\n",
       " 'emptiness': 342,\n",
       " 'defiled': 343,\n",
       " 'until': 344,\n",
       " 'goon': 345,\n",
       " 'biscuit.': 346,\n",
       " 'door': 347,\n",
       " 'tyrant,': 348,\n",
       " 'washed': 349,\n",
       " 'starveling': 350,\n",
       " 'discomfort,': 351,\n",
       " 'very': 352,\n",
       " 'already,': 353,\n",
       " 'norfolk,': 354,\n",
       " 'steaming': 355,\n",
       " 'five': 356,\n",
       " 'conversation,': 357,\n",
       " 'thrown': 358,\n",
       " 'wrong': 359,\n",
       " 'us,': 360,\n",
       " 'branches': 361,\n",
       " 'off': 362,\n",
       " 'does': 363,\n",
       " 'come': 364,\n",
       " 'everyone': 365,\n",
       " 'ankles': 366,\n",
       " 'between': 367,\n",
       " 'hospital.': 368,\n",
       " 'authorities': 369,\n",
       " 'majors': 370,\n",
       " 'think': 371,\n",
       " 'habit': 372,\n",
       " 'should': 373,\n",
       " 'types': 374,\n",
       " 'policy,': 375,\n",
       " 'and,': 376,\n",
       " 'cheap,': 377,\n",
       " 'sardine-tins': 378,\n",
       " 'rubbish,': 379,\n",
       " 'fell': 380,\n",
       " 'soldierly': 381,\n",
       " 'skulking': 382,\n",
       " 'room,': 383,\n",
       " 'bolt': 384,\n",
       " 'speculation': 385,\n",
       " 'betrayed': 386,\n",
       " 'police': 387,\n",
       " 'given': 388,\n",
       " 'sparse': 389,\n",
       " 'shirt-clad': 390,\n",
       " 'good': 391,\n",
       " 'step,': 392,\n",
       " 'endure': 393,\n",
       " 'with': 394,\n",
       " 'once': 395,\n",
       " 'now': 396,\n",
       " 'hardly': 397,\n",
       " 'worst': 398,\n",
       " 'snow.': 399,\n",
       " 'consolations': 400,\n",
       " 'the': 401,\n",
       " 'accent': 402,\n",
       " 'we': 403,\n",
       " 'much': 404,\n",
       " 'been': 405,\n",
       " 'no': 406,\n",
       " 'most': 407,\n",
       " 'steam': 408,\n",
       " 'hairy,': 409,\n",
       " 'kitchen.': 410,\n",
       " 'treat': 411,\n",
       " 'there.': 412,\n",
       " 'wastage': 413,\n",
       " 'looked': 414,\n",
       " 'built': 415,\n",
       " 'cells.': 416,\n",
       " 'bit': 417,\n",
       " 'pillow.': 418,\n",
       " 'doing': 419,\n",
       " 'vague': 420,\n",
       " 'chaining': 421,\n",
       " \"so,'\": 422,\n",
       " 'usual,': 423,\n",
       " 'eyes': 424,\n",
       " 'might': 425,\n",
       " 'than': 426,\n",
       " 'connived': 427,\n",
       " 'themselves,': 428,\n",
       " 'tramps,': 429,\n",
       " 'blasphemous,': 430,\n",
       " 'really': 431,\n",
       " 'greasy': 432,\n",
       " 'left': 433,\n",
       " 'prison': 434,\n",
       " 'below': 435,\n",
       " 'against': 436,\n",
       " 'makings': 437,\n",
       " 'woman,': 438,\n",
       " 'in': 439,\n",
       " 'from': 440,\n",
       " 'take': 441,\n",
       " 'iniquities': 442,\n",
       " 'bursting': 443,\n",
       " 'pints': 444,\n",
       " 'daddy,': 445,\n",
       " 'everlasting': 446,\n",
       " 'respect.': 447,\n",
       " 'peached': 448,\n",
       " 'bits': 449,\n",
       " 'instructive': 450,\n",
       " 'feet': 451,\n",
       " 'spring,': 452,\n",
       " 'up.': 453,\n",
       " 'ran': 454,\n",
       " 'figures': 455,\n",
       " 'paper': 456,\n",
       " 'just': 457,\n",
       " 'grass,': 458,\n",
       " 'notorious': 459,\n",
       " 'get': 460,\n",
       " 'room': 461,\n",
       " 'overhead': 462,\n",
       " 'me,': 463,\n",
       " 'bathed': 464,\n",
       " 'better': 465,\n",
       " 'hairy': 466,\n",
       " 'gulp': 467,\n",
       " 'move': 468,\n",
       " 'outside': 469,\n",
       " 'army.': 470,\n",
       " 'dog.': 471,\n",
       " 'slipped': 472,\n",
       " 'cockney': 473,\n",
       " 'its': 474,\n",
       " 'twelve': 475,\n",
       " 'three': 476,\n",
       " 'suffer': 477,\n",
       " 'leg.': 478,\n",
       " 'discoloured,': 479,\n",
       " 'kicked': 480,\n",
       " 'us': 481,\n",
       " 'bluish': 482,\n",
       " 'where': 483,\n",
       " 'their': 484,\n",
       " 'many': 485,\n",
       " 'nudity,': 486,\n",
       " 'invariable': 487,\n",
       " 'spike.': 488,\n",
       " 'lighting': 489,\n",
       " 'bugs,': 490,\n",
       " 'discovered': 491,\n",
       " 'roller': 492,\n",
       " 'terrible': 493,\n",
       " 'done': 494,\n",
       " 'comfortless': 495,\n",
       " 'numbers': 496,\n",
       " 'biggest': 497,\n",
       " 'cold.': 498,\n",
       " 'show': 499,\n",
       " 'bell': 500,\n",
       " 'press': 501,\n",
       " 'old': 502,\n",
       " 'days': 503,\n",
       " 'unlettered': 504,\n",
       " 'upside': 505,\n",
       " 'hid': 506,\n",
       " 'sunday-morning': 507,\n",
       " 'bella,': 508,\n",
       " 'rents': 509,\n",
       " 'your': 510,\n",
       " \"that's\": 511,\n",
       " 'too': 512,\n",
       " 'physical': 513,\n",
       " 'could': 514,\n",
       " 'spent': 515,\n",
       " 'devil,': 516,\n",
       " 'on,': 517,\n",
       " 'sweaty': 518,\n",
       " 'meals': 519,\n",
       " 'parson': 520,\n",
       " 'toes.': 521,\n",
       " '(which': 522,\n",
       " 'indecent': 523,\n",
       " 'drove': 524,\n",
       " 'string': 525,\n",
       " 'seventy-four,': 526,\n",
       " 'bawling,': 527,\n",
       " 'horrors': 528,\n",
       " 'uncharitable': 529,\n",
       " 'covered': 530,\n",
       " 'ornament': 531,\n",
       " 'promptly': 532,\n",
       " 'answer.': 533,\n",
       " 'rather': 534,\n",
       " 'tight': 535,\n",
       " 'riff-raff.': 536,\n",
       " 'beggar': 537,\n",
       " 'passed,': 538,\n",
       " 'sired': 539,\n",
       " 'himself,': 540,\n",
       " 'hollow': 541,\n",
       " 'gloomy,': 542,\n",
       " 'giving': 543,\n",
       " 'falling': 544,\n",
       " 'always': 545,\n",
       " 'piece': 546,\n",
       " 'crumpled': 547,\n",
       " 'exhaustedly,': 548,\n",
       " 'blank,': 549,\n",
       " 'there,': 550,\n",
       " 'miscall': 551,\n",
       " 'myself,': 552,\n",
       " 'peculiar': 553,\n",
       " 'beyond': 554,\n",
       " 'yards': 555,\n",
       " 'lighted': 556,\n",
       " 'more': 557,\n",
       " \"'you\": 558,\n",
       " 'fall': 559,\n",
       " 'primitive': 560,\n",
       " 'unlocking': 561,\n",
       " 'tiny,': 562,\n",
       " 'william': 563,\n",
       " 'service.': 564,\n",
       " 'bathroom,': 565,\n",
       " 'imagine,': 566,\n",
       " 'soul': 567,\n",
       " 'for': 568,\n",
       " 'stay': 569,\n",
       " 'do;': 570,\n",
       " 'lime-washed,': 571,\n",
       " 'any': 572,\n",
       " 'coveted': 573,\n",
       " 'shut': 574,\n",
       " 'christ,': 575,\n",
       " 'at': 576,\n",
       " 'luckier': 577,\n",
       " 'hat,': 578,\n",
       " 'beard': 579,\n",
       " 'clarity.': 580,\n",
       " 'man,': 581,\n",
       " 'said,': 582,\n",
       " 'forty,': 583,\n",
       " 'tin': 584,\n",
       " 'evening,': 585,\n",
       " 'toff,': 586,\n",
       " 'scum': 587,\n",
       " \"o'clock\": 588,\n",
       " 'dingy,': 589,\n",
       " 'cell': 590,\n",
       " 'immediately,': 591,\n",
       " 'herald,': 592,\n",
       " 'pint': 593,\n",
       " 'fifty-seven': 594,\n",
       " 'dun': 595,\n",
       " 'when': 596,\n",
       " 'motionless': 597,\n",
       " 'anyone': 598,\n",
       " 'inflamed': 599,\n",
       " 'evils,': 600,\n",
       " 'next': 601,\n",
       " 'here,': 602,\n",
       " 'all,': 603,\n",
       " 'make': 604,\n",
       " 'is': 605,\n",
       " 'glasgow.': 606,\n",
       " 'bow': 607,\n",
       " 'tommy': 608,\n",
       " 'pound': 609,\n",
       " 'other': 610,\n",
       " \"'daddy',\": 611,\n",
       " 'confinement.': 612,\n",
       " 'fred,': 613,\n",
       " 'socially': 614,\n",
       " 'supper': 615,\n",
       " 'would': 616,\n",
       " 'patches,': 617,\n",
       " 'inspection,': 618,\n",
       " 'law)': 619,\n",
       " 'did': 620,\n",
       " 'unless': 621,\n",
       " 'sickly,': 622,\n",
       " 'what': 623,\n",
       " 'high': 624,\n",
       " 'shock': 625,\n",
       " 'eyes,': 626,\n",
       " 'imagine': 627,\n",
       " 'evils.': 628,\n",
       " 'duty': 629,\n",
       " 'ignorant': 630,\n",
       " 'glance': 631,\n",
       " 'pay,': 632,\n",
       " 'barred': 633,\n",
       " 'leaves': 634,\n",
       " 'great': 635,\n",
       " 'has': 636,\n",
       " 'quite': 637,\n",
       " 'sunday.': 638,\n",
       " 'on': 639,\n",
       " 'head': 640,\n",
       " 'hot': 641,\n",
       " 'mind.': 642,\n",
       " 'sky.': 643,\n",
       " 'thigh.': 644,\n",
       " 'bent': 645,\n",
       " 'threatening': 646,\n",
       " 'glad': 647,\n",
       " 'told': 648,\n",
       " 'penalties': 649,\n",
       " 'refused': 650,\n",
       " 'ranks': 651,\n",
       " 'though': 652,\n",
       " 'stuffed': 653,\n",
       " 'parcel': 654,\n",
       " 'money': 655,\n",
       " 'palliasses,': 656,\n",
       " 'window': 657,\n",
       " 'by.': 658,\n",
       " 'have': 659,\n",
       " 'hungry': 660,\n",
       " 'into': 661,\n",
       " 'floor.': 662,\n",
       " 'ceremony': 663,\n",
       " 'unspeakably': 664,\n",
       " 'dining-room': 665,\n",
       " 'urban': 666,\n",
       " 'go': 667,\n",
       " 'searched,': 668,\n",
       " 'at,': 669,\n",
       " 'except': 670,\n",
       " 'herded': 671,\n",
       " 'begun,': 672,\n",
       " 'i': 673,\n",
       " 'they': 674,\n",
       " 'officially': 675,\n",
       " 'like': 676,\n",
       " 'top': 677,\n",
       " 'silly': 678,\n",
       " 'lives': 679,\n",
       " 'turning': 680,\n",
       " 'frozen,': 681,\n",
       " 'food,': 682,\n",
       " 'do': 683,\n",
       " 'sundays,': 684,\n",
       " 'floor,': 685,\n",
       " 'decided': 686,\n",
       " 'unwritten': 687,\n",
       " 'sleeps': 688,\n",
       " 'hours.': 689,\n",
       " 'hours': 690,\n",
       " 'cold,': 691,\n",
       " 'year,': 692,\n",
       " 'hundred': 693,\n",
       " 'see': 694,\n",
       " 'family': 695,\n",
       " 'eight': 696,\n",
       " 'bread,': 697,\n",
       " 'shortcoming': 698,\n",
       " 'wall,': 699,\n",
       " 'almost': 700,\n",
       " 'chilly,': 701,\n",
       " 'caught': 702,\n",
       " 'about': 703,\n",
       " 'breakfast,': 704,\n",
       " 'taken': 705,\n",
       " 'counties,': 706,\n",
       " 'consecutive': 707,\n",
       " 'bill': 708,\n",
       " 'boredom': 709,\n",
       " 'came': 710,\n",
       " 'watching': 711,\n",
       " 'while': 712,\n",
       " 'faces.': 713,\n",
       " 'days.': 714,\n",
       " 'dire': 715,\n",
       " 'horrid,': 716,\n",
       " 'truly': 717,\n",
       " 'used': 718,\n",
       " 'perhaps': 719,\n",
       " 'minutes': 720,\n",
       " 'were.': 721,\n",
       " 'dog': 722,\n",
       " 'luck,': 723,\n",
       " 'hither': 724,\n",
       " 'boozers,': 725,\n",
       " 'stove': 726,\n",
       " 'swung': 727,\n",
       " 'can': 728,\n",
       " 'pleased': 729,\n",
       " 'meal': 730,\n",
       " 'nothing': 731,\n",
       " 'minds.': 732,\n",
       " 'sole': 733,\n",
       " 'occupying': 734,\n",
       " 'ever': 735,\n",
       " 'sunday,': 736,\n",
       " \"night's\": 737,\n",
       " 'burning': 738,\n",
       " 'cigarettes': 739,\n",
       " 'gossip': 740,\n",
       " 'sat': 741,\n",
       " 'put': 742,\n",
       " 'bathroom.': 743,\n",
       " 'such': 744,\n",
       " 'me.': 745,\n",
       " 'gave': 746,\n",
       " 'constant': 747,\n",
       " 'open': 748,\n",
       " 'tobacco,': 749,\n",
       " 'waking': 750,\n",
       " 'scrubby': 751,\n",
       " 'blossom,': 752,\n",
       " 'impounded.': 753,\n",
       " 'met': 754,\n",
       " 'dirty': 755,\n",
       " 'feet,': 756,\n",
       " 'smell.': 757,\n",
       " 'george,': 758,\n",
       " 'two': 759,\n",
       " 'dripping': 760,\n",
       " 'hedge,': 761,\n",
       " 'you': 762,\n",
       " 'heads,': 763,\n",
       " 'side': 764,\n",
       " 'aged': 765,\n",
       " 'another': 766,\n",
       " 'somebody.': 767,\n",
       " 'sleeping': 768,\n",
       " 'gate.': 769,\n",
       " 'know': 770,\n",
       " 'bored': 771,\n",
       " \"couldn't\": 772,\n",
       " 'greatly': 773,\n",
       " 'decently': 774,\n",
       " 'then': 775,\n",
       " 'shed': 776,\n",
       " 'picture:': 777,\n",
       " 'drivelled,': 778,\n",
       " 'belly': 779,\n",
       " 'names': 780,\n",
       " 'imagined': 781,\n",
       " 'tramp': 782,\n",
       " 'slippery': 783,\n",
       " 'thereafter': 784,\n",
       " 'dull': 785,\n",
       " 'something': 786,\n",
       " 'our': 787,\n",
       " 'spike,': 788,\n",
       " 'age,': 789,\n",
       " 'cotton': 790,\n",
       " 'as': 791,\n",
       " 'time': 792,\n",
       " 'sovereigns.': 793,\n",
       " 'this': 794,\n",
       " 'tired': 795,\n",
       " 'schoolboys': 796,\n",
       " 'caught.': 797,\n",
       " 'song': 798,\n",
       " 'some': 799,\n",
       " 'grumbled': 800,\n",
       " 'curs': 801,\n",
       " 'spyhole': 802,\n",
       " 'smoked': 803,\n",
       " 'arrived': 804,\n",
       " 'so': 805,\n",
       " 'rare': 806,\n",
       " 'had': 807,\n",
       " 'bind': 808,\n",
       " 'flint.': 809,\n",
       " 'served.': 810,\n",
       " 'open.': 811,\n",
       " 'not': 812,\n",
       " 'things': 813,\n",
       " 'spike': 814,\n",
       " 'lay': 815,\n",
       " 'seemed': 816,\n",
       " 'major': 817,\n",
       " 'if': 818,\n",
       " 'seeing': 819,\n",
       " 'spikes,': 820,\n",
       " 'wooden': 821,\n",
       " 'asleep': 822,\n",
       " 'flabby': 823,\n",
       " 'heavy': 824,\n",
       " 'dinner': 825,\n",
       " 'boards': 826,\n",
       " 'forbidden.': 827,\n",
       " 'day,': 828,\n",
       " 'long': 829,\n",
       " 'mere': 830,\n",
       " 'an': 831,\n",
       " 'dustbins': 832,\n",
       " 'dining-room,': 833,\n",
       " 'own': 834,\n",
       " 'these;': 835,\n",
       " 'these': 836,\n",
       " 'sub-faecal': 837,\n",
       " 'slowly': 838,\n",
       " 'bloody': 839,\n",
       " 'them': 840,\n",
       " 'you.': 841,\n",
       " 'hole': 842,\n",
       " 'dipping-pond,': 843,\n",
       " 'bathroom': 844,\n",
       " 'dreary': 845,\n",
       " 'layers': 846,\n",
       " 'were,': 847,\n",
       " 'agonies': 848,\n",
       " 'of': 849,\n",
       " 'shirts,': 850,\n",
       " 'naked': 851,\n",
       " 'resourceless': 852,\n",
       " 'so-called': 853,\n",
       " 'shuffled': 854,\n",
       " 'down': 855,\n",
       " 'bread': 856,\n",
       " 'nude.': 857,\n",
       " 'tartar,': 858,\n",
       " 'filtered': 859,\n",
       " 'men': 860,\n",
       " 'himself.': 861,\n",
       " 'perhapsâ€”the': 862,\n",
       " 'passage': 863,\n",
       " 'bastard': 864,\n",
       " 'door.': 865,\n",
       " 'rules': 866,\n",
       " 'soon': 867,\n",
       " 'dishes': 868,\n",
       " 'remained.': 869,\n",
       " 'sticking': 870,\n",
       " 'holes,': 871,\n",
       " 'helping': 872,\n",
       " 'called': 873,\n",
       " 'hour': 874,\n",
       " 'be': 875,\n",
       " 'anything': 876,\n",
       " 'furtively,': 877,\n",
       " 'much.': 878,\n",
       " 'blankets': 879,\n",
       " 'by': 880,\n",
       " 'official': 881,\n",
       " 'precaution': 882,\n",
       " 'woolly': 883,\n",
       " 'both.': 884,\n",
       " 'road,': 885,\n",
       " 'nightshirts,': 886,\n",
       " 'deal': 887,\n",
       " 'sent': 888,\n",
       " 'lost': 889,\n",
       " 'me': 890,\n",
       " 'served': 891,\n",
       " 'under': 892,\n",
       " \"gentleman?'\": 893,\n",
       " 'morning,': 894,\n",
       " 'hide': 895,\n",
       " 'poverty': 896,\n",
       " 'searched.': 897,\n",
       " 'particulars': 898,\n",
       " 'whose': 899,\n",
       " 'bemuse': 900,\n",
       " 'supposed': 901,\n",
       " 'sturdy': 902,\n",
       " 'bucketfuls': 903,\n",
       " 'sternest': 904,\n",
       " 'held': 905,\n",
       " 'eightpence': 906,\n",
       " 'thing,': 907,\n",
       " 'forbidden': 908,\n",
       " 'suppose': 909,\n",
       " 'ends': 910,\n",
       " 'night.': 911,\n",
       " 'look.': 912,\n",
       " 'green': 913,\n",
       " \"is.'\": 914,\n",
       " 'queer': 915,\n",
       " 'casual': 916,\n",
       " 'boots,': 917,\n",
       " 'cigarette': 918,\n",
       " 'furniture': 919,\n",
       " 'or': 920,\n",
       " 'stood': 921,\n",
       " 'kept': 922,\n",
       " 'barrel,': 923,\n",
       " \"'toe-rags',\": 924,\n",
       " 'doctor': 925,\n",
       " 'middle': 926,\n",
       " 'towels': 927,\n",
       " 'unhappy': 928,\n",
       " 'law': 929,\n",
       " 'rest.': 930,\n",
       " 'so,': 931,\n",
       " 'may': 932,\n",
       " 'tea-leaves.': 933,\n",
       " 'are': 934,\n",
       " 'clouts': 935,\n",
       " 'world': 936,\n",
       " 'copy': 937,\n",
       " 'cells': 938,\n",
       " 'sagging': 939,\n",
       " 'confine': 940,\n",
       " 'six': 941,\n",
       " 'up': 942,\n",
       " 'paradise': 943,\n",
       " 'believe': 944,\n",
       " 'sunburn.': 945,\n",
       " 'scotty,': 946,\n",
       " 'luxuries': 947,\n",
       " 'rushing': 948,\n",
       " 'few': 949,\n",
       " 'outside,': 950,\n",
       " 'going': 951,\n",
       " 'twice': 952,\n",
       " 'among': 953,\n",
       " 'matches': 954,\n",
       " 'cheeks,': 955,\n",
       " 'elephantiasis.': 956,\n",
       " 'bundles': 957,\n",
       " 'half,': 958,\n",
       " 'comfortable': 959,\n",
       " 'chestnut': 960,\n",
       " 'became': 961,\n",
       " 'shop.': 962,\n",
       " 'point': 963,\n",
       " 'knee,': 964,\n",
       " 'sitting': 965,\n",
       " 'overflowing': 966,\n",
       " 'shook': 967,\n",
       " 'undress': 968,\n",
       " 'buttons,': 969,\n",
       " \"you'd\": 970,\n",
       " 'restless': 971,\n",
       " 'raffles': 972,\n",
       " 'place,': 973,\n",
       " 'storing': 974,\n",
       " 'washing-up,': 975,\n",
       " 'them.': 976,\n",
       " 'it.': 977,\n",
       " 'last': 978,\n",
       " 'also,': 979,\n",
       " 'tramps.': 980,\n",
       " \"'for\": 981,\n",
       " 'herring-gutted': 982,\n",
       " 'fixed': 983,\n",
       " 'gorged': 984,\n",
       " 'luck': 985,\n",
       " 'bathe': 986,\n",
       " 'over': 987,\n",
       " 'oaths': 988,\n",
       " 'narrow': 989,\n",
       " 'sheep': 990,\n",
       " 'to': 991,\n",
       " 'gate': 992,\n",
       " 'upon': 993,\n",
       " 'cannot': 994,\n",
       " 'thither': 995,\n",
       " 'mooching,': 996,\n",
       " 'day': 997,\n",
       " \"'don't\": 998,\n",
       " 'there': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char2idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "95f79e93-3d6e-4004-99c7-e744e3611d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_pad_sequences_pytorch(sequences, max_len):\n",
    "    padded_sequences = []\n",
    "    \n",
    "    for seq in sequences:\n",
    "        # If the sequence is shorter than max_len, pad with zeros at the beginning\n",
    "        if len(seq) < max_len:\n",
    "            padded_seq = [0] * (max_len - len(seq)) + seq  # Pre-padding with 0\n",
    "        # If the sequence is longer than max_len, truncate it\n",
    "        else:\n",
    "            padded_seq = seq[-max_len:]  # Keep the last max_len elements\n",
    "        padded_sequences.append(padded_seq)\n",
    "    \n",
    "    return torch.tensor(padded_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "0fcc34bf-cf12-4245-9f40-9f0d7b0974b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_padded = pre_pad_sequences_pytorch(X_train, max_len)\n",
    "X_train_padded=X_train_padded.unsqueeze(-1)\n",
    "Y_train=torch.tensor(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "0e7d40d1-966a-4ddf-bef1-336079845142",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "213"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "f4b5819a-b797-49dc-ad80-4579c3950fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "class LSTM:\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size, output_size):\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = torch.randn(vocab_size, embedding_dim, requires_grad=True)\n",
    "\n",
    "        # Initialize weights with requires_grad=True\n",
    "        self.Wf = torch.randn(hidden_size, embedding_dim + hidden_size, requires_grad=True)\n",
    "        self.bf = torch.zeros(hidden_size, requires_grad=True)\n",
    "        \n",
    "        self.Wi = torch.randn(hidden_size, embedding_dim + hidden_size, requires_grad=True)\n",
    "        self.bi = torch.zeros(hidden_size, requires_grad=True)\n",
    "        \n",
    "        self.Wc = torch.randn(hidden_size, embedding_dim + hidden_size, requires_grad=True)\n",
    "        self.bc = torch.zeros(hidden_size, requires_grad=True)\n",
    "        \n",
    "        self.Wo = torch.randn(hidden_size, embedding_dim + hidden_size, requires_grad=True)\n",
    "        self.bo = torch.zeros(hidden_size, requires_grad=True)\n",
    "\n",
    "        self.Wv = torch.randn(output_size, hidden_size, requires_grad=True)\n",
    "        self.bv = torch.zeros(output_size, requires_grad=True)\n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        nn.init.xavier_uniform_(self.Wf)\n",
    "        nn.init.xavier_uniform_(self.Wi)\n",
    "        nn.init.xavier_uniform_(self.Wc)\n",
    "        nn.init.xavier_uniform_(self.Wo)\n",
    "        nn.init.xavier_uniform_(self.Wv)\n",
    "\n",
    "\n",
    "    def parameters(self):\n",
    "        # Return a list of all parameters (weights and biases) in the model\n",
    "        return [self.Wf, self.bf, self.Wi, self.bi, self.Wc, self.bc, self.Wo, self.bo, self.Wv, self.bv, self.embedding]\n",
    "\n",
    "    def forward(self, x, init_states=None):\n",
    "    # Apply embedding layer to input indices\n",
    "        # print(f\"X shape coming in model.forward{x.shape}\")\n",
    "        x=x.squeeze(dim=-1)\n",
    "        # print(x.shape)\n",
    "        x = self.embedding[x] \n",
    "        # print(f\"X shape coming after embedding{x.shape}\")\n",
    "        \n",
    "        # print(x.shape)\n",
    "        # Shape: (batch_size, seq_len, embedding_dim)\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "    \n",
    "    # Initialize h_t and c_t if init_states is None\n",
    "        if init_states is None:\n",
    "            h_t = torch.zeros(batch_size, self.hidden_size, requires_grad=True)\n",
    "            c_t = torch.zeros(batch_size, self.hidden_size, requires_grad=True)\n",
    "        else:\n",
    "            h_t, c_t = init_states\n",
    "        \n",
    "        outputs = []\n",
    "        for t in range(seq_len):\n",
    "            x_t = x[:, t, :]  # Shape: (batch_size, embedding_dim)\n",
    "            \n",
    "            Z_t = torch.cat([x_t, h_t], dim=1)  # Shape: (batch_size, embedding_dim + hidden_size)\n",
    "            # print(Z_t.shape)\n",
    "            # break\n",
    "            # Forget gate\n",
    "            ft = torch.sigmoid(Z_t @ self.Wf.t() + self.bf)\n",
    "            # Input gate\n",
    "            it = torch.sigmoid(Z_t @ self.Wi.t() + self.bi)\n",
    "            # Candidate cell state\n",
    "            can = torch.tanh(Z_t @ self.Wc.t() + self.bc)\n",
    "            # Output gate\n",
    "            ot = torch.sigmoid(Z_t @ self.Wo.t() + self.bo)\n",
    "        \n",
    "            \n",
    "            c_t = c_t * ft + can * it\n",
    "            \n",
    "            h_t = ot * torch.tanh(c_t)\n",
    "        \n",
    "            # Compute output for current time step\n",
    "            y_t = h_t @ self.Wv.t() + self.bv\n",
    "            # outputs.append(y_t)\n",
    "        \n",
    "        # Stack outputs across the time dimension (seq_len)\n",
    "        # outputs = torch.stack(outputs, dim=1)\n",
    "        return y_t, (h_t, c_t)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "b18713b9-dac1-4863-9d07-f66ff2bc2678",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTM(vocab_size=vocab_size, embedding_dim=10, hidden_size=128, output_size=vocab_size)\n",
    "params = model.parameters()\n",
    "optimizer = torch.optim.Adam(params, lr=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "4ef7f4a5-540d-41b2-8b79-e2b9842dd0c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6.1963, grad_fn=<NllLossBackward0>)\n",
      "tensor(6.1273, grad_fn=<NllLossBackward0>)\n",
      "tensor(6.2829, grad_fn=<NllLossBackward0>)\n",
      "tensor(6.2121, grad_fn=<NllLossBackward0>)\n",
      "tensor(6.3325, grad_fn=<NllLossBackward0>)\n",
      "tensor(6.1679, grad_fn=<NllLossBackward0>)\n",
      "tensor(6.3374, grad_fn=<NllLossBackward0>)\n",
      "tensor(6.1030, grad_fn=<NllLossBackward0>)\n",
      "tensor(6.4398, grad_fn=<NllLossBackward0>)\n",
      "tensor(6.0408, grad_fn=<NllLossBackward0>)\n",
      "tensor(6.3122, grad_fn=<NllLossBackward0>)\n",
      "tensor(6.4605, grad_fn=<NllLossBackward0>)\n",
      "tensor(6.1222, grad_fn=<NllLossBackward0>)\n",
      "tensor(6.1336, grad_fn=<NllLossBackward0>)\n",
      "tensor(6.2812, grad_fn=<NllLossBackward0>)\n",
      "tensor(5.9523, grad_fn=<NllLossBackward0>)\n",
      "tensor(5.8275, grad_fn=<NllLossBackward0>)\n",
      "tensor(6.3046, grad_fn=<NllLossBackward0>)\n",
      "tensor(6.1705, grad_fn=<NllLossBackward0>)\n",
      "tensor(6.0598, grad_fn=<NllLossBackward0>)\n",
      "tensor(6.1097, grad_fn=<NllLossBackward0>)\n",
      "tensor(6.0187, grad_fn=<NllLossBackward0>)\n",
      "tensor(5.7385, grad_fn=<NllLossBackward0>)\n",
      "tensor(6.1399, grad_fn=<NllLossBackward0>)\n",
      "tensor(6.1378, grad_fn=<NllLossBackward0>)\n",
      "tensor(6.0227, grad_fn=<NllLossBackward0>)\n",
      "tensor(5.9183, grad_fn=<NllLossBackward0>)\n",
      "tensor(5.9248, grad_fn=<NllLossBackward0>)\n",
      "tensor(6.1367, grad_fn=<NllLossBackward0>)\n",
      "tensor(5.7647, grad_fn=<NllLossBackward0>)\n",
      "tensor(5.9958, grad_fn=<NllLossBackward0>)\n",
      "tensor(6.0414, grad_fn=<NllLossBackward0>)\n",
      "tensor(5.7113, grad_fn=<NllLossBackward0>)\n",
      "tensor(6.0540, grad_fn=<NllLossBackward0>)\n",
      "tensor(6.0634, grad_fn=<NllLossBackward0>)\n",
      "tensor(5.8874, grad_fn=<NllLossBackward0>)\n",
      "tensor(6.0701, grad_fn=<NllLossBackward0>)\n",
      "tensor(5.9373, grad_fn=<NllLossBackward0>)\n",
      "tensor(6.0635, grad_fn=<NllLossBackward0>)\n",
      "tensor(5.6747, grad_fn=<NllLossBackward0>)\n",
      "tensor(5.8707, grad_fn=<NllLossBackward0>)\n",
      "tensor(5.8843, grad_fn=<NllLossBackward0>)\n",
      "tensor(5.6499, grad_fn=<NllLossBackward0>)\n",
      "tensor(5.6175, grad_fn=<NllLossBackward0>)\n",
      "tensor(5.8537, grad_fn=<NllLossBackward0>)\n",
      "tensor(5.7838, grad_fn=<NllLossBackward0>)\n",
      "tensor(5.4643, grad_fn=<NllLossBackward0>)\n",
      "tensor(5.8510, grad_fn=<NllLossBackward0>)\n",
      "tensor(5.7428, grad_fn=<NllLossBackward0>)\n",
      "tensor(5.4265, grad_fn=<NllLossBackward0>)\n",
      "tensor(5.8503, grad_fn=<NllLossBackward0>)\n",
      "tensor(5.8988, grad_fn=<NllLossBackward0>)\n",
      "tensor(5.9167, grad_fn=<NllLossBackward0>)\n",
      "tensor(5.7035, grad_fn=<NllLossBackward0>)\n",
      "tensor(5.6514, grad_fn=<NllLossBackward0>)\n",
      "tensor(5.9784, grad_fn=<NllLossBackward0>)\n",
      "tensor(5.6389, grad_fn=<NllLossBackward0>)\n",
      "tensor(5.4880, grad_fn=<NllLossBackward0>)\n",
      "tensor(5.9829, grad_fn=<NllLossBackward0>)\n",
      "tensor(5.7486, grad_fn=<NllLossBackward0>)\n",
      "tensor(6.0649, grad_fn=<NllLossBackward0>)\n",
      "tensor(5.6476, grad_fn=<NllLossBackward0>)\n",
      "tensor(5.8057, grad_fn=<NllLossBackward0>)\n",
      "tensor(5.6325, grad_fn=<NllLossBackward0>)\n",
      "tensor(5.4575, grad_fn=<NllLossBackward0>)\n",
      "tensor(5.4227, grad_fn=<NllLossBackward0>)\n",
      "tensor(5.9047, grad_fn=<NllLossBackward0>)\n",
      "tensor(5.7232, grad_fn=<NllLossBackward0>)\n",
      "tensor(5.5835, grad_fn=<NllLossBackward0>)\n",
      "tensor(5.8122, grad_fn=<NllLossBackward0>)\n",
      "tensor(5.7501, grad_fn=<NllLossBackward0>)\n",
      "tensor(5.3247, grad_fn=<NllLossBackward0>)\n",
      "tensor(5.1719, grad_fn=<NllLossBackward0>)\n",
      "tensor(5.3540, grad_fn=<NllLossBackward0>)\n",
      "tensor(5.5860, grad_fn=<NllLossBackward0>)\n",
      "tensor(5.2613, grad_fn=<NllLossBackward0>)\n",
      "tensor(5.4465, grad_fn=<NllLossBackward0>)\n",
      "tensor(5.4844, grad_fn=<NllLossBackward0>)\n",
      "tensor(5.3595, grad_fn=<NllLossBackward0>)\n",
      "tensor(5.6895, grad_fn=<NllLossBackward0>)\n",
      "tensor(5.5800, grad_fn=<NllLossBackward0>)\n",
      "tensor(5.2613, grad_fn=<NllLossBackward0>)\n",
      "tensor(5.1021, grad_fn=<NllLossBackward0>)\n",
      "tensor(5.3638, grad_fn=<NllLossBackward0>)\n",
      "tensor(5.4029, grad_fn=<NllLossBackward0>)\n",
      "tensor(5.4612, grad_fn=<NllLossBackward0>)\n",
      "tensor(5.3232, grad_fn=<NllLossBackward0>)\n",
      "tensor(5.0952, grad_fn=<NllLossBackward0>)\n",
      "tensor(5.3363, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.9587, grad_fn=<NllLossBackward0>)\n",
      "tensor(5.3404, grad_fn=<NllLossBackward0>)\n",
      "tensor(5.1986, grad_fn=<NllLossBackward0>)\n",
      "tensor(5.2243, grad_fn=<NllLossBackward0>)\n",
      "tensor(5.1914, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.7558, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.9270, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.9175, grad_fn=<NllLossBackward0>)\n",
      "tensor(5.0234, grad_fn=<NllLossBackward0>)\n",
      "tensor(5.2618, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.8516, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.8058, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.7470, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.8045, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.9521, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.8578, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.8828, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.8026, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.7738, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.7769, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.6140, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.8064, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.5001, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.7112, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.4489, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.4377, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.6626, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.2871, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.3003, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.1260, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.3562, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.4170, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.1631, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.1422, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.2197, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.2574, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.0513, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.2327, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.0554, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.0700, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.0563, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.1004, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.0879, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.2148, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.9798, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.6677, grad_fn=<NllLossBackward0>)\n",
      "tensor(4.3193, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7323, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.6653, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.8658, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7458, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7520, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.6429, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.6558, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.7880, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.6973, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.6687, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.5839, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.5141, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.4247, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.5396, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.3633, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.4498, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.3033, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.3228, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.5805, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.3570, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2582, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2941, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9413, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.3447, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0245, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0455, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0767, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1854, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8786, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8587, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7836, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9704, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0036, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9907, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8151, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8106, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9744, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9156, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4990, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9136, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6051, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9707, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7101, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6149, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4757, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4619, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5460, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6892, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5553, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4791, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6028, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5602, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3258, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5838, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3375, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.4199, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2360, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2638, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.2863, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3612, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.3863, grad_fn=<NllLossBackward0>)\n",
      "tensor(1.9366, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.0129, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.0471, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "hidden_state = None  # Initialize hidden state\n",
    "for _ in range(200):\n",
    "    # Sample a batch\n",
    "    batch_indices = torch.randint(0, X_train_padded.shape[0], (128,))\n",
    "    x_train = X_train_padded[batch_indices]  # Shape: (batch_size, seq_len)\n",
    "    \n",
    "    # Forward pass\n",
    "    # print(f\"x_train passed into forward{x_train.shape}\")\n",
    "    outputs,hidden_state = model.forward(x_train, init_states=hidden_state)\n",
    "    # print(outputs.shape)\n",
    "    h_t, c_t = hidden_state\n",
    "    hidden_state = (h_t.detach(), c_t.detach())  # Detach hidden state for next batch\n",
    "\n",
    "    # Compute loss\n",
    "    y_batch = Y_train[batch_indices]  # Shape: (batch_size, seq_len, vocab_size)\n",
    "    loss = criterion(outputs, y_batch)\n",
    "    \n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Print loss\n",
    "    print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "9ce3771f-7679-4204-a07f-e929c40c39aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text:\n",
      "scum hurried our ankles the dining-room and of us and to the waist of christ, mate,' the food was the may set and we had eightpence in our cigarettes with tobacco to the spike and then and him the stuff and\n"
     ]
    }
   ],
   "source": [
    "def generate_sequence(model, seed_string, char2idx, idx2char, sequence_length, max_len=55):\n",
    "    \"\"\"\n",
    "    Generates a sequence from the trained LSTM model given a seed string.\n",
    "\n",
    "    Args:\n",
    "        model: Trained LSTM model.\n",
    "        seed_string: The initial string to seed the sequence generation.\n",
    "        char2idx: Dictionary mapping characters to indices.\n",
    "        idx2char: Dictionary mapping indices to characters.\n",
    "        sequence_length: Number of tokens to generate.\n",
    "        max_len: Maximum length of input sequence for the model.\n",
    "\n",
    "    Returns:\n",
    "        Generated sequence as a string.\n",
    "    \"\"\"\n",
    "    # model.eval()  # Set the model to evaluation mode\n",
    "    \n",
    "    # Convert seed string to sequence of indices\n",
    "    seed_indices = [char2idx[word] for word in seed_string.split(\" \") if word in char2idx]\n",
    "    seed_tensor = torch.tensor(seed_indices).unsqueeze(0)  # Shape: (1, seq_len)\n",
    "    \n",
    "    generated_indices = seed_indices[:]  # Start with the seed sequence\n",
    "    hidden_state = None  # Initialize hidden state\n",
    "\n",
    "    for _ in range(sequence_length):\n",
    "        # Pre-pad the input sequence to match the model's expected input size\n",
    "        padded_input = pre_pad_sequences_pytorch([generated_indices], max_len).unsqueeze(-1)\n",
    "        # print(padded_input.shape)\n",
    "        # Get the model output and hidden state\n",
    "        output, (hidden_state) = model.forward(padded_input, hidden_state)\n",
    "        # print(output.shape)\n",
    "        \n",
    "        # Take the output corresponding to the last token\n",
    "        next_token_logits = output  # Shape: (1, vocab_size)\n",
    "        \n",
    "        # Use softmax to get probabilities and sample the next token\n",
    "        next_token_prob = torch.softmax(next_token_logits, dim=-1)\n",
    "        # next_token_idx = torch.multinomial(next_token_prob, num_samples=1).item()\n",
    "        next_token_idx=torch.argmax(next_token_prob).item()\n",
    "        \n",
    "        # Append the predicted token to the sequence\n",
    "        generated_indices.append(next_token_idx)\n",
    "    \n",
    "    # Convert indices back to characters\n",
    "    generated_words = [idx2char[idx] for idx in generated_indices]\n",
    "    return \" \".join(generated_words)\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "seed_string = \"scum\"\n",
    "sequence_length = 40\n",
    "generated_text = generate_sequence(model, seed_string, char2idx, idx2char, sequence_length)\n",
    "print(\"Generated Text:\")\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9d7a8fe2-b75f-4968-8e54-277f4cf8073f",
   "metadata": {},
   "source": [
    "Generated Text:\n",
    "It is.' if bath, is.' if bath, cockney side, black serve is.' go three asleep straw bath, is.' cotton when when This up apparatus kind where Majors tub a stripped eight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec74bf7b-3a65-4818-9a5a-fe07763b737e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Generated Text:\n",
    "scum was set forty-eight men one men among us, forty-eight and left cold, was worse tub the it tobacco and its of the would washing-up, and both. was astonishing; One me two cold There It was off and the prison The room. was a set in a lime-washed, tramp pot-bellied, degenerate curs we looked. Shock heads, hairy, crumpled faces, hollow chests, flat feet, sagging muscles—every kind of malformation and physical rottenness were there. All were flabby and discoloured, as all tramps are under their deceptive sunburn. Two or three figures wen there stay ineradicably in my mind. Old 'Daddy', aged seventy-four,"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "agrim",
   "language": "python",
   "name": "agrim"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
